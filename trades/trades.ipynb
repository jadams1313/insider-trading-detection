{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6249cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lstm as lstm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908afa3",
   "metadata": {},
   "source": [
    "Configs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3606b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_file = r'data\\input\\media_integrated\\media_stock_features.csv'\n",
    "company = 'HLF'\n",
    "seq_len = 50\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d209b08",
   "metadata": {},
   "source": [
    "Process Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91be702",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = lstm.EnhancedLSTMPipeline(\n",
    "    integrated_file=integrated_file,\n",
    "    seq_len=seq_len,\n",
    "    company=company\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b98b3",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c90d0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training model for AMSC (code: 1)\n",
      "======================================================================\n",
      "Found 2737 records for AMSC\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 303ms/step - loss: 0.5836 - val_loss: 0.3417\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.4115 - val_loss: 0.3455\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - loss: 0.2778 - val_loss: 0.4701\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - loss: 0.4000 - val_loss: 0.3478\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - loss: 0.4445 - val_loss: 0.3478\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - loss: 0.2867 - val_loss: 0.3542\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 191ms/step - loss: 0.3812 - val_loss: 0.3522\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - loss: 0.2904 - val_loss: 0.3475\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - loss: 0.3326 - val_loss: 0.3541\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - loss: 0.4234 - val_loss: 0.3489\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step - loss: 0.2922 - val_loss: 0.3509\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step - loss: 0.2818 - val_loss: 0.4020\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - loss: 0.3167 - val_loss: 0.3484\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step - loss: 0.3568 - val_loss: 0.3564\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - loss: 0.2792 - val_loss: 0.3448\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - loss: 0.3655 - val_loss: 0.3440\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 0.2794 - val_loss: 0.3814\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - loss: 0.3317 - val_loss: 0.3414\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - loss: 0.2805 - val_loss: 0.4003\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step - loss: 0.2894 - val_loss: 0.3394\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/amsc_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/amsc_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/amsc_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 70.5s\n",
      "\n",
      "======================================================================\n",
      "Training model for BP (code: 2)\n",
      "======================================================================\n",
      "Found 2737 records for BP\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 403ms/step - loss: 1.4803 - val_loss: 1.5813\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - loss: 1.0058 - val_loss: 1.4880\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - loss: 0.6892 - val_loss: 1.3879\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.8422 - val_loss: 1.4020\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.7511 - val_loss: 1.4007\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - loss: 0.9538 - val_loss: 1.2908\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - loss: 0.7274 - val_loss: 1.3792\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - loss: 0.6420 - val_loss: 1.3438\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - loss: 0.6821 - val_loss: 1.2896\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.6584 - val_loss: 1.1938\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.6715 - val_loss: 1.1972\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - loss: 0.5857 - val_loss: 1.1361\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - loss: 1.0201 - val_loss: 1.1851\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - loss: 0.7483 - val_loss: 1.1001\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - loss: 0.8565 - val_loss: 1.0732\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.7207 - val_loss: 1.1932\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - loss: 1.0103 - val_loss: 1.0630\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - loss: 0.8689 - val_loss: 1.0312\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - loss: 0.6717 - val_loss: 1.0668\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276ms/step - loss: 0.8987 - val_loss: 1.0405\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/bp_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/bp_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/bp_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 75.9s\n",
      "\n",
      "======================================================================\n",
      "Training model for EVR (code: 3)\n",
      "======================================================================\n",
      "Found 2737 records for EVR\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 351ms/step - loss: 0.4998 - val_loss: 0.3280\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.3559 - val_loss: 0.3074\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - loss: 0.3250 - val_loss: 0.3150\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.3000 - val_loss: 0.3007\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.2833 - val_loss: 0.2982\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - loss: 0.2924 - val_loss: 0.3109\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - loss: 0.2934 - val_loss: 0.2978\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - loss: 0.2721 - val_loss: 0.3190\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.3292 - val_loss: 0.2950\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.2936 - val_loss: 0.3008\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.3183 - val_loss: 0.2904\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.2894 - val_loss: 0.2867\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279ms/step - loss: 0.2860 - val_loss: 0.2887\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - loss: 0.2827 - val_loss: 0.2933\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.2600 - val_loss: 0.2884\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.2372 - val_loss: 0.2868\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - loss: 0.2480 - val_loss: 0.2841\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.2641 - val_loss: 0.2941\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step - loss: 0.2685 - val_loss: 0.2842\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 276ms/step - loss: 0.2723 - val_loss: 0.2810\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/evr_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/evr_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/evr_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 73.9s\n",
      "\n",
      "======================================================================\n",
      "Training model for GOOGL (code: 4)\n",
      "======================================================================\n",
      "Found 2737 records for GOOGL\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 356ms/step - loss: 2.2572 - val_loss: 0.6294\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - loss: 2.9325 - val_loss: 0.6162\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step - loss: 2.6478 - val_loss: 0.5765\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step - loss: 2.3302 - val_loss: 0.5689\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 3.1020 - val_loss: 0.6096\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step - loss: 2.0243 - val_loss: 0.5589\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 2.5051 - val_loss: 0.5777\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285ms/step - loss: 1.5788 - val_loss: 0.5797\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 1.7827 - val_loss: 0.5877\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step - loss: 2.2300 - val_loss: 0.5402\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 1.8989 - val_loss: 0.6024\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step - loss: 2.8550 - val_loss: 0.5501\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 2.1705 - val_loss: 0.5289\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 2.0910 - val_loss: 0.5589\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 2.2821 - val_loss: 0.5304\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 3.2613 - val_loss: 0.5221\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - loss: 1.8083 - val_loss: 0.5159\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 2.0112 - val_loss: 0.5122\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 2.2892 - val_loss: 0.5216\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 2.2836 - val_loss: 0.5208\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/googl_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/googl_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/googl_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 73.8s\n",
      "\n",
      "======================================================================\n",
      "Training model for HLF (code: 5)\n",
      "======================================================================\n",
      "Found 2736 records for HLF\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2736\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-17 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2735 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1878 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - loss: 1.3720 - val_loss: 15421968.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 0.9314 - val_loss: 15421841.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 1.1327 - val_loss: 15421825.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - loss: 0.8940 - val_loss: 15421839.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.8423 - val_loss: 15421781.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step - loss: 0.8584 - val_loss: 15421549.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.9948 - val_loss: 15421516.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242ms/step - loss: 1.2474 - val_loss: 15421582.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.9927 - val_loss: 15421747.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.8098 - val_loss: 15421576.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.8880 - val_loss: 15421587.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step - loss: 0.9114 - val_loss: 15421609.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.9358 - val_loss: 15421459.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.8842 - val_loss: 15421649.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.8463 - val_loss: 15421607.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step - loss: 0.7759 - val_loss: 15421734.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.9054 - val_loss: 15421653.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - loss: 1.0712 - val_loss: 15421737.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.9027 - val_loss: 15421740.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.6830 - val_loss: 15421497.0000\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/hlf_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/hlf_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/hlf_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 72.2s\n",
      "\n",
      "======================================================================\n",
      "Training model for MDRX (code: 6)\n",
      "======================================================================\n",
      "Found 2737 records for MDRX\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 346ms/step - loss: 0.5594 - val_loss: 1.1042\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.4146 - val_loss: 1.0275\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238ms/step - loss: 0.4003 - val_loss: 1.0320\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.3648 - val_loss: 0.9614\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234ms/step - loss: 0.3747 - val_loss: 0.9741\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238ms/step - loss: 0.3456 - val_loss: 0.9264\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.3333 - val_loss: 0.9117\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - loss: 0.3433 - val_loss: 0.9936\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - loss: 0.3686 - val_loss: 0.8801\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 0.3838 - val_loss: 0.9052\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.3216 - val_loss: 0.8526\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.3190 - val_loss: 0.8473\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - loss: 0.3021 - val_loss: 0.8487\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - loss: 0.3261 - val_loss: 0.8389\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - loss: 0.3835 - val_loss: 0.8480\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - loss: 0.3597 - val_loss: 0.8100\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - loss: 0.3196 - val_loss: 0.7992\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - loss: 0.3222 - val_loss: 0.7919\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.3313 - val_loss: 0.7903\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - loss: 0.3551 - val_loss: 0.7816\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/mdrx_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/mdrx_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/mdrx_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 80.9s\n",
      "\n",
      "======================================================================\n",
      "Training model for ORCL (code: 7)\n",
      "======================================================================\n",
      "Found 2737 records for ORCL\n",
      "\n",
      "======================================================================\n",
      "Loading data - BASELINE (volume only)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 1 features: ['Volume']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 326ms/step - loss: 0.4206 - val_loss: 0.3987\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.2987 - val_loss: 0.3876\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.3350 - val_loss: 0.3954\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 0.2747 - val_loss: 0.3944\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.2640 - val_loss: 0.3783\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - loss: 0.2619 - val_loss: 0.3873\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242ms/step - loss: 0.2819 - val_loss: 0.3727\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.2475 - val_loss: 0.3688\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.2403 - val_loss: 0.3782\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - loss: 0.2396 - val_loss: 0.3631\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - loss: 0.2657 - val_loss: 0.3733\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.2270 - val_loss: 0.3559\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.2536 - val_loss: 0.3681\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.2446 - val_loss: 0.3534\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 0.2125 - val_loss: 0.3703\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.2458 - val_loss: 0.3492\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 0.2358 - val_loss: 0.3487\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.2148 - val_loss: 0.3484\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step - loss: 0.2161 - val_loss: 0.3432\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.2409 - val_loss: 0.3496\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/orcl_full_window_pred_baseline.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/orcl_full_point_pred_baseline.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/orcl_full_sequence_pred_baseline.csv\n",
      "\n",
      "BASELINE model complete! Time: 73.7s\n",
      "\n",
      "======================================================================\n",
      "Training model for WFC (code: 8)\n",
      "======================================================================\n",
      "WARNING: No data found for ticker WFC (code 8)\n"
     ]
    }
   ],
   "source": [
    "tickers = {1: \"AMSC\", 2: \"BP\", 3: \"EVR\", 4:\"GOOGL\", 5 : \"HLF\", 6:\"MDRX\", 7: \"ORCL\", 8:\"WFC\"}\n",
    "\n",
    "full_df = pd.read_csv(r'data\\input\\media_integrated\\media_stock_features.csv')\n",
    "\n",
    "# Ticker mapping\n",
    "##tickers = {0: \"AMSC\", 1: \"BP\", 2: \"EVR\", 3: \"GOOGL\", 4: \"HLF\", 5: \"MDRX\", 6: \"ORCL\", 7: \"WFC\"}\n",
    "\n",
    "# Dictionary to store models for each ticker\n",
    "models_baseline = {}\n",
    "histories_baseline = {}\n",
    "models_enhanced = {}\n",
    "histories_baseline = {}\n",
    "# Create temp directory if it doesn't exist\n",
    "os.makedirs('data/temp', exist_ok=True)\n",
    "\n",
    "# Train a separate model for each ticker\n",
    "for ticker_code, ticker_name in tickers.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training model for {ticker_name} (code: {ticker_code})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Filter data for this ticker and save to temporary file\n",
    "    ticker_data = full_df[full_df['Ticker'] == ticker_code].copy()\n",
    "    \n",
    "    if len(ticker_data) == 0:\n",
    "        print(f\"WARNING: No data found for ticker {ticker_name} (code {ticker_code})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(ticker_data)} records for {ticker_name}\")\n",
    "    \n",
    "    # Save to temporary file\n",
    "    temp_file = f'data/temp/{ticker_name}_processed.csv'\n",
    "    ticker_data.to_csv(temp_file, index=False)\n",
    "    \n",
    "    # Create pipeline for this ticker\n",
    "    pipeline = lstm.EnhancedLSTMPipeline(\n",
    "        integrated_file=temp_file,\n",
    "        seq_len=50,\n",
    "        company=ticker_name.lower()\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model_baseline, history_baseline = pipeline.train_and_predict(\n",
    "        use_gdelt=False, \n",
    "        epochs=20\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    models_baseline[ticker_name] = model_baseline\n",
    "    histories_baseline[ticker_name] = history_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f5e41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training model for AMSC (code: 1)\n",
      "======================================================================\n",
      "Found 2737 records for AMSC\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 353ms/step - loss: 0.5888 - val_loss: 0.3482\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.4218 - val_loss: 0.3928\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - loss: 0.3653 - val_loss: 0.3613\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step - loss: 0.3078 - val_loss: 0.3635\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.2978 - val_loss: 0.4346\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 0.4034 - val_loss: 0.3549\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.3597 - val_loss: 0.3607\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 0.3442 - val_loss: 0.3649\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.3077 - val_loss: 0.3518\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.4010 - val_loss: 0.3713\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - loss: 0.4276 - val_loss: 0.3504\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - loss: 0.3194 - val_loss: 0.3642\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 0.3155 - val_loss: 0.3528\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.3054 - val_loss: 0.3549\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.4194 - val_loss: 0.3505\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.3682 - val_loss: 0.3509\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.3960 - val_loss: 0.3664\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - loss: 0.2836 - val_loss: 0.3465\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 295ms/step - loss: 0.2895 - val_loss: 0.3694\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 0.2765 - val_loss: 0.3578\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/amsc_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/amsc_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/amsc_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 73.3s\n",
      "\n",
      "======================================================================\n",
      "Training model for BP (code: 2)\n",
      "======================================================================\n",
      "Found 2737 records for BP\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 666ms/step - loss: 1.1085 - val_loss: 1.6387\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - loss: 1.0569 - val_loss: 1.5546\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 0.8228 - val_loss: 1.5117\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.6637 - val_loss: 1.4228\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step - loss: 1.0920 - val_loss: 1.3993\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.6042 - val_loss: 1.3221\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step - loss: 0.6853 - val_loss: 1.3078\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step - loss: 0.8860 - val_loss: 1.2719\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.9915 - val_loss: 1.2538\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.7151 - val_loss: 1.2442\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 0.6868 - val_loss: 1.2009\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 0.7050 - val_loss: 1.1769\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - loss: 1.0320 - val_loss: 1.1652\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.8500 - val_loss: 1.1504\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257ms/step - loss: 0.9610 - val_loss: 1.1182\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.9997 - val_loss: 1.0945\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.8658 - val_loss: 1.1211\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.6085 - val_loss: 1.1112\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - loss: 0.9161 - val_loss: 1.0684\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - loss: 0.6634 - val_loss: 1.0228\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/bp_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/bp_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/bp_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 76.1s\n",
      "\n",
      "======================================================================\n",
      "Training model for EVR (code: 3)\n",
      "======================================================================\n",
      "Found 2737 records for EVR\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 361ms/step - loss: 0.6081 - val_loss: 0.3434\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - loss: 0.3765 - val_loss: 0.3103\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - loss: 0.3020 - val_loss: 0.3436\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - loss: 0.3513 - val_loss: 0.3119\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.3262 - val_loss: 0.3059\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 0.3071 - val_loss: 0.3097\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.3440 - val_loss: 0.3079\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - loss: 0.3038 - val_loss: 0.2991\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 0.2740 - val_loss: 0.3001\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - loss: 0.2926 - val_loss: 0.3201\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - loss: 0.3053 - val_loss: 0.3036\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - loss: 0.2653 - val_loss: 0.3229\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.3036 - val_loss: 0.2919\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - loss: 0.2778 - val_loss: 0.2984\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - loss: 0.2583 - val_loss: 0.2912\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.2657 - val_loss: 0.3059\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.2811 - val_loss: 0.2890\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.2703 - val_loss: 0.2907\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.2903 - val_loss: 0.2978\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - loss: 0.2355 - val_loss: 0.2856\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/evr_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/evr_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/evr_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 74.8s\n",
      "\n",
      "======================================================================\n",
      "Training model for GOOGL (code: 4)\n",
      "======================================================================\n",
      "Found 2737 records for GOOGL\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 339ms/step - loss: 3.6164 - val_loss: 0.5917\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273ms/step - loss: 1.5665 - val_loss: 0.6374\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 2.9771 - val_loss: 0.6314\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 2.5090 - val_loss: 0.5875\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 3.0955 - val_loss: 0.5831\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 3.2070 - val_loss: 0.5820\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 1.6732 - val_loss: 0.6585\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 1.8698 - val_loss: 0.5631\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - loss: 2.1175 - val_loss: 0.5525\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - loss: 1.7374 - val_loss: 0.5508\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 1.9668 - val_loss: 0.5576\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 2.8964 - val_loss: 0.5410\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 1.9509 - val_loss: 0.5438\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 1.9224 - val_loss: 0.5540\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 2.0334 - val_loss: 0.5337\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - loss: 2.9450 - val_loss: 0.5651\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step - loss: 2.3989 - val_loss: 0.5338\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 2.4493 - val_loss: 0.5310\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step - loss: 2.3190 - val_loss: 0.5225\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - loss: 1.7052 - val_loss: 0.5850\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/googl_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/googl_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/googl_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 73.8s\n",
      "\n",
      "======================================================================\n",
      "Training model for HLF (code: 5)\n",
      "======================================================================\n",
      "Found 2736 records for HLF\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2736\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-17 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2735 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1878 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 597ms/step - loss: 1.2223 - val_loss: 15422009.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - loss: 1.2371 - val_loss: 15422000.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.9770 - val_loss: 15421905.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.6855 - val_loss: 15421843.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261ms/step - loss: 0.8650 - val_loss: 15421849.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.8038 - val_loss: 15421706.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 1.1520 - val_loss: 15421923.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.9699 - val_loss: 15421846.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - loss: 0.9550 - val_loss: 15421812.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267ms/step - loss: 0.9906 - val_loss: 15421579.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - loss: 0.8414 - val_loss: 15421748.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - loss: 0.7210 - val_loss: 15421802.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.9908 - val_loss: 15421756.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - loss: 0.9902 - val_loss: 15421959.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 244ms/step - loss: 0.9257 - val_loss: 15421864.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - loss: 0.9364 - val_loss: 15421769.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - loss: 0.9165 - val_loss: 15421751.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - loss: 0.9010 - val_loss: 15421823.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 1.0299 - val_loss: 15421794.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - loss: 0.9191 - val_loss: 15421817.0000\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/hlf_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/hlf_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/hlf_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 73.9s\n",
      "\n",
      "======================================================================\n",
      "Training model for MDRX (code: 6)\n",
      "======================================================================\n",
      "Found 2737 records for MDRX\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 318ms/step - loss: 0.6014 - val_loss: 1.1949\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.4067 - val_loss: 1.0563\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - loss: 0.3691 - val_loss: 1.0247\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - loss: 0.4197 - val_loss: 0.9797\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.3838 - val_loss: 1.0174\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - loss: 0.3769 - val_loss: 0.9945\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - loss: 0.3629 - val_loss: 1.0413\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - loss: 0.4171 - val_loss: 0.9349\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.3209 - val_loss: 0.9322\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - loss: 0.3378 - val_loss: 0.9467\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.3494 - val_loss: 0.9646\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - loss: 0.4372 - val_loss: 0.9122\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - loss: 0.2883 - val_loss: 0.8672\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - loss: 0.3769 - val_loss: 0.8841\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step - loss: 0.3670 - val_loss: 0.8764\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - loss: 0.3498 - val_loss: 0.8650\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.3823 - val_loss: 0.8760\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - loss: 0.3362 - val_loss: 0.8405\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step - loss: 0.3330 - val_loss: 0.8917\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - loss: 0.3072 - val_loss: 0.8086\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/mdrx_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/mdrx_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/mdrx_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 73.8s\n",
      "\n",
      "======================================================================\n",
      "Training model for ORCL (code: 7)\n",
      "======================================================================\n",
      "Found 2737 records for ORCL\n",
      "\n",
      "======================================================================\n",
      "Loading data - ENHANCED (with GDELT)\n",
      "======================================================================\n",
      "Total observations: 2737\n",
      "Date range: 2015-01-02 05:00:00 to 2025-11-18 05:00:00\n",
      "Using 4 features: ['Volume', 'ArticleCount', 'Tone', 'Polarity']\n",
      "After removing NaN: 2736 samples\n",
      "\n",
      "Data split:\n",
      "  Training: 1879 samples\n",
      "  Testing: 537 samples\n",
      "  Validation: 269 samples\n",
      "\n",
      "Building LSTM model...\n",
      "\n",
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367ms/step - loss: 0.6306 - val_loss: 0.4497\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 0.3428 - val_loss: 0.4419\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - loss: 0.3179 - val_loss: 0.4041\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - loss: 0.3224 - val_loss: 0.3876\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - loss: 0.2731 - val_loss: 0.3922\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - loss: 0.2856 - val_loss: 0.3892\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323ms/step - loss: 0.2717 - val_loss: 0.3865\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334ms/step - loss: 0.2763 - val_loss: 0.3823\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285ms/step - loss: 0.2679 - val_loss: 0.3805\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - loss: 0.2968 - val_loss: 0.4258\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - loss: 0.2825 - val_loss: 0.3931\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.2884 - val_loss: 0.3759\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 0.2480 - val_loss: 0.3692\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 0.2811 - val_loss: 0.3708\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - loss: 0.2604 - val_loss: 0.3778\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - loss: 0.2648 - val_loss: 0.3666\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - loss: 0.2508 - val_loss: 0.3631\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - loss: 0.2191 - val_loss: 0.3569\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step - loss: 0.2626 - val_loss: 0.3581\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 295ms/step - loss: 0.2248 - val_loss: 0.3616\n",
      "\n",
      "Generating predictions for all three methods...\n",
      "  1. Window-based forecasting...\n",
      "  Saved: data/output/orcl_full_window_pred_enhanced.csv\n",
      "  2. Day-ahead forecasting...\n",
      "  Saved: data/output/orcl_full_point_pred_enhanced.csv\n",
      "  3. Historical-based forecasting...\n",
      "  Saved: data/output/orcl_full_sequence_pred_enhanced.csv\n",
      "\n",
      "ENHANCED model complete! Time: 77.6s\n",
      "\n",
      "======================================================================\n",
      "Training model for WFC (code: 8)\n",
      "======================================================================\n",
      "WARNING: No data found for ticker WFC (code 8)\n"
     ]
    }
   ],
   "source": [
    "tickers = {1: \"AMSC\", 2: \"BP\", 3: \"EVR\", 4:\"GOOGL\", 5 : \"HLF\", 6:\"MDRX\", 7: \"ORCL\", 8:\"WFC\"}\n",
    "\n",
    "full_df = pd.read_csv(r'data\\input\\media_integrated\\media_stock_features.csv')\n",
    "\n",
    "# Ticker mapping\n",
    "##tickers = {0: \"AMSC\", 1: \"BP\", 2: \"EVR\", 3: \"GOOGL\", 4: \"HLF\", 5: \"MDRX\", 6: \"ORCL\", 7: \"WFC\"}\n",
    "\n",
    "# Dictionary to store models for each ticker\n",
    "\n",
    "models_enhanced = {}\n",
    "histories_enhanced = {}\n",
    "# Create temp directory if it doesn't exist\n",
    "os.makedirs('data/temp', exist_ok=True)\n",
    "\n",
    "# Train a separate model for each ticker\n",
    "for ticker_code, ticker_name in tickers.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training model for {ticker_name} (code: {ticker_code})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Filter data for this ticker and save to temporary file\n",
    "    ticker_data = full_df[full_df['Ticker'] == ticker_code].copy()\n",
    "    \n",
    "    if len(ticker_data) == 0:\n",
    "        print(f\"WARNING: No data found for ticker {ticker_name} (code {ticker_code})\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(ticker_data)} records for {ticker_name}\")\n",
    "    \n",
    "    # Save to temporary file\n",
    "    temp_file = f'data/temp/{ticker_name}_processed.csv'\n",
    "    ticker_data.to_csv(temp_file, index=False)\n",
    "    \n",
    "    # Create pipeline for this ticker\n",
    "    pipeline = lstm.EnhancedLSTMPipeline(\n",
    "        integrated_file=temp_file,\n",
    "        seq_len=50,\n",
    "        company=ticker_name.lower()\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    '''model_baseline, history_baseline = pipeline.train_and_predict(\n",
    "        use_gdelt=True, \n",
    "        epochs=20\n",
    "    )'''\n",
    "\n",
    "    model_enhanced, history_enhanced = pipeline.train_and_predict(\n",
    "        use_gdelt=True, \n",
    "        epochs=20\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    '''models_baseline[ticker_name] = model_baseline\n",
    "    histories_baseline[ticker_name] = history_baseline'''\n",
    "    models_enhanced[ticker_name] = model_enhanced\n",
    "    histories_baseline[ticker_name] = history_enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b3add",
   "metadata": {},
   "source": [
    "Comparison between companies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ae32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "            'window': 'Window-Based Forecasting',\n",
    "            'point': 'Day-Ahead Forecasting',\n",
    "            'sequence': 'Historical-Based Forecasting'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33c228bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPANY: AMSC\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0202\n",
      "  RMSE:      0.612686\n",
      "  MAE:       0.405466\n",
      "  MAPE:      125.47%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.4724\n",
      "  RMSE:      0.440574\n",
      "  MAE:       0.280161\n",
      "  MAPE:      624.68%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0202\n",
      "  RMSE:      0.612686\n",
      "  MAE:       0.405466\n",
      "  MAPE:      125.47%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -15.7082\n",
      "  RMSE:      2.479425\n",
      "  MAE:       1.919569\n",
      "  MAPE:      2220.37%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.4786\n",
      "  RMSE:      0.438015\n",
      "  MAE:       0.273694\n",
      "  MAPE:      667.10%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -15.7082\n",
      "  RMSE:      2.479425\n",
      "  MAE:       1.919569\n",
      "  MAPE:      2220.37%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: BP\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0072\n",
      "  RMSE:      0.966984\n",
      "  MAE:       0.617710\n",
      "  MAPE:      374.42%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5514\n",
      "  RMSE:      0.645337\n",
      "  MAE:       0.381115\n",
      "  MAPE:      204.93%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0072\n",
      "  RMSE:      0.966984\n",
      "  MAE:       0.617710\n",
      "  MAPE:      374.42%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -2.0693\n",
      "  RMSE:      1.688058\n",
      "  MAE:       1.345432\n",
      "  MAPE:      1923.48%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5429\n",
      "  RMSE:      0.651409\n",
      "  MAE:       0.387923\n",
      "  MAPE:      370.51%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -2.0693\n",
      "  RMSE:      1.688058\n",
      "  MAE:       1.345432\n",
      "  MAPE:      1923.48%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: EVR\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0342\n",
      "  RMSE:      0.702974\n",
      "  MAE:       0.414742\n",
      "  MAPE:      418.71%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.4985\n",
      "  RMSE:      0.489505\n",
      "  MAE:       0.273142\n",
      "  MAPE:      691.74%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0342\n",
      "  RMSE:      0.702974\n",
      "  MAE:       0.414742\n",
      "  MAPE:      418.71%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.1333\n",
      "  RMSE:      0.735868\n",
      "  MAE:       0.464969\n",
      "  MAPE:      816.24%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5018\n",
      "  RMSE:      0.487919\n",
      "  MAE:       0.282322\n",
      "  MAPE:      827.71%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.1333\n",
      "  RMSE:      0.735868\n",
      "  MAE:       0.464969\n",
      "  MAPE:      816.24%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: GOOGL\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.5777\n",
      "  RMSE:      2.002313\n",
      "  MAE:       1.523584\n",
      "  MAPE:      1401.55%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.2928\n",
      "  RMSE:      1.340556\n",
      "  MAE:       0.549966\n",
      "  MAPE:      602.09%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.5777\n",
      "  RMSE:      2.002313\n",
      "  MAE:       1.523584\n",
      "  MAPE:      1401.55%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0441\n",
      "  RMSE:      1.628932\n",
      "  MAE:       0.748077\n",
      "  MAPE:      169.47%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.2844\n",
      "  RMSE:      1.348479\n",
      "  MAE:       0.636867\n",
      "  MAPE:      916.05%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0441\n",
      "  RMSE:      1.628932\n",
      "  MAE:       0.748077\n",
      "  MAPE:      169.47%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: HLF\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  0.0007\n",
      "  RMSE:      14.506652\n",
      "  MAE:       2.498308\n",
      "  MAPE:      1045.15%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.0220\n",
      "  RMSE:      14.351137\n",
      "  MAE:       1.529973\n",
      "  MAPE:      378.52%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  0.0007\n",
      "  RMSE:      14.506652\n",
      "  MAE:       2.498308\n",
      "  MAPE:      1045.15%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0028\n",
      "  RMSE:      14.532319\n",
      "  MAE:       1.818559\n",
      "  MAPE:      308.82%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.0185\n",
      "  RMSE:      14.377068\n",
      "  MAE:       1.488717\n",
      "  MAPE:      291.79%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0028\n",
      "  RMSE:      14.532319\n",
      "  MAE:       1.818559\n",
      "  MAPE:      308.82%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: MDRX\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0264\n",
      "  RMSE:      1.039149\n",
      "  MAE:       0.596701\n",
      "  MAPE:      287.88%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.3874\n",
      "  RMSE:      0.802792\n",
      "  MAE:       0.395447\n",
      "  MAPE:      373.98%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0264\n",
      "  RMSE:      1.039149\n",
      "  MAE:       0.596701\n",
      "  MAPE:      287.88%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0834\n",
      "  RMSE:      1.067646\n",
      "  MAE:       0.578024\n",
      "  MAPE:      116.64%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.3558\n",
      "  RMSE:      0.823267\n",
      "  MAE:       0.449698\n",
      "  MAPE:      521.28%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0834\n",
      "  RMSE:      1.067646\n",
      "  MAE:       0.578024\n",
      "  MAPE:      116.64%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: ORCL\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.4709\n",
      "  RMSE:      0.853607\n",
      "  MAE:       0.550036\n",
      "  MAPE:      584.84%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5266\n",
      "  RMSE:      0.484262\n",
      "  MAE:       0.271508\n",
      "  MAPE:      294.13%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.4709\n",
      "  RMSE:      0.853607\n",
      "  MAE:       0.550036\n",
      "  MAPE:      584.84%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.0631\n",
      "  RMSE:      0.725692\n",
      "  MAE:       0.458685\n",
      "  MAPE:      122.46%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5011\n",
      "  RMSE:      0.497125\n",
      "  MAE:       0.275395\n",
      "  MAPE:      261.66%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.0631\n",
      "  RMSE:      0.725692\n",
      "  MAE:       0.458685\n",
      "  MAPE:      122.46%\n",
      "\n",
      "======================================================================\n",
      "COMPANY: WFC\n",
      "======================================================================\n",
      "\n",
      "=== Results for baseline model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.2541\n",
      "  RMSE:      0.929697\n",
      "  MAE:       0.634627\n",
      "  MAPE:      254.19%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.5040\n",
      "  RMSE:      0.584698\n",
      "  MAE:       0.373401\n",
      "  MAPE:      285.73%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.2541\n",
      "  RMSE:      0.929697\n",
      "  MAE:       0.634627\n",
      "  MAPE:      254.19%\n",
      "\n",
      "=== Results for enhanced model ===\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Score:  -0.1729\n",
      "  RMSE:      0.899097\n",
      "  MAE:       0.612791\n",
      "  MAPE:      187.93%\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Score:  0.4586\n",
      "  RMSE:      0.610874\n",
      "  MAE:       0.384861\n",
      "  MAPE:      269.38%\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Score:  -0.1729\n",
      "  RMSE:      0.899097\n",
      "  MAE:       0.612791\n",
      "  MAPE:      187.93%\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "results = []\n",
    "\n",
    "for company in tickers.values():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPANY: {company}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize metrics dictionary for this company\n",
    "    if company not in metrics:\n",
    "        metrics[company] = {}\n",
    "    \n",
    "    for model_type in ['baseline', 'enhanced']:\n",
    "        print(f\"\\n=== Results for {model_type} model ===\")\n",
    "        \n",
    "        # Initialize model type dictionary\n",
    "        if model_type not in metrics[company]:\n",
    "            metrics[company][model_type] = {}\n",
    "        \n",
    "        for method, method_name in methods.items():\n",
    "            print(f\"\\n{method_name}:\")\n",
    "            \n",
    "            pred_file = f'data/output/{company}_full_{method}_pred_{model_type}.csv'\n",
    "            actual_file = f'data/output/{company}_full_{method}_act.csv'\n",
    "            \n",
    "            predictions = pd.read_csv(pred_file, header=None).values.flatten()\n",
    "            actuals = pd.read_csv(actual_file, header=None).values.flatten()\n",
    "            \n",
    "            # Calculate metrics for this method\n",
    "            method_metrics = pipeline.calculate_metrics(predictions, actuals)\n",
    "            metrics[company][model_type][method] = method_metrics\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"  R² Score:  {method_metrics['R2']:.4f}\")\n",
    "            print(f\"  RMSE:      {method_metrics['RMSE']:.6f}\")\n",
    "            print(f\"  MAE:       {method_metrics['MAE']:.6f}\")\n",
    "            print(f\"  MAPE:      {method_metrics['MAPE']:.2f}%\")\n",
    "            \n",
    "            # Store results\n",
    "            model_results = {\n",
    "                'Company': company,\n",
    "                'Model': model_type,\n",
    "                'Method': method,\n",
    "                'Method_Name': method_name,\n",
    "                'Predictions': predictions,\n",
    "                'Actuals': actuals,\n",
    "                'R2': method_metrics['R2'],\n",
    "                'RMSE': method_metrics['RMSE'],\n",
    "                'MAE': method_metrics['MAE'],\n",
    "                'MAPE': method_metrics['MAPE']\n",
    "            }\n",
    "            results.append(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806cdc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVEMENTS FOR AMSC\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: -77505.82% (Baseline: -0.0202 → Enhanced: -15.7082)\n",
      "  RMSE Improvement: -304.68% (Baseline: 0.612686 → Enhanced: 2.479425)\n",
      "  MAE Improvement: -373.42% (Baseline: 0.405466 → Enhanced: 1.919569)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: +1.29% (Baseline: 0.4724 → Enhanced: 0.4786)\n",
      "  RMSE Improvement: +0.58% (Baseline: 0.440574 → Enhanced: 0.438015)\n",
      "  MAE Improvement: +2.31% (Baseline: 0.280161 → Enhanced: 0.273694)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: -77505.82% (Baseline: -0.0202 → Enhanced: -15.7082)\n",
      "  RMSE Improvement: -304.68% (Baseline: 0.612686 → Enhanced: 2.479425)\n",
      "  MAE Improvement: -373.42% (Baseline: 0.405466 → Enhanced: 1.919569)\n",
      "IMPROVEMENTS FOR BP\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: -28716.73% (Baseline: -0.0072 → Enhanced: -2.0693)\n",
      "  RMSE Improvement: -74.57% (Baseline: 0.966984 → Enhanced: 1.688058)\n",
      "  MAE Improvement: -117.81% (Baseline: 0.617710 → Enhanced: 1.345432)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -1.54% (Baseline: 0.5514 → Enhanced: 0.5429)\n",
      "  RMSE Improvement: -0.94% (Baseline: 0.645337 → Enhanced: 0.651409)\n",
      "  MAE Improvement: -1.79% (Baseline: 0.381115 → Enhanced: 0.387923)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: -28716.73% (Baseline: -0.0072 → Enhanced: -2.0693)\n",
      "  RMSE Improvement: -74.57% (Baseline: 0.966984 → Enhanced: 1.688058)\n",
      "  MAE Improvement: -117.81% (Baseline: 0.617710 → Enhanced: 1.345432)\n",
      "IMPROVEMENTS FOR EVR\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: -289.61% (Baseline: -0.0342 → Enhanced: -0.1333)\n",
      "  RMSE Improvement: -4.68% (Baseline: 0.702974 → Enhanced: 0.735868)\n",
      "  MAE Improvement: -12.11% (Baseline: 0.414742 → Enhanced: 0.464969)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: +0.65% (Baseline: 0.4985 → Enhanced: 0.5018)\n",
      "  RMSE Improvement: +0.32% (Baseline: 0.489505 → Enhanced: 0.487919)\n",
      "  MAE Improvement: -3.36% (Baseline: 0.273142 → Enhanced: 0.282322)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: -289.61% (Baseline: -0.0342 → Enhanced: -0.1333)\n",
      "  RMSE Improvement: -4.68% (Baseline: 0.702974 → Enhanced: 0.735868)\n",
      "  MAE Improvement: -12.11% (Baseline: 0.414742 → Enhanced: 0.464969)\n",
      "IMPROVEMENTS FOR GOOGL\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: +92.36% (Baseline: -0.5777 → Enhanced: -0.0441)\n",
      "  RMSE Improvement: +18.65% (Baseline: 2.002313 → Enhanced: 1.628932)\n",
      "  MAE Improvement: +50.90% (Baseline: 1.523584 → Enhanced: 0.748077)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -2.86% (Baseline: 0.2928 → Enhanced: 0.2844)\n",
      "  RMSE Improvement: -0.59% (Baseline: 1.340556 → Enhanced: 1.348479)\n",
      "  MAE Improvement: -15.80% (Baseline: 0.549966 → Enhanced: 0.636867)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: +92.36% (Baseline: -0.5777 → Enhanced: -0.0441)\n",
      "  RMSE Improvement: +18.65% (Baseline: 2.002313 → Enhanced: 1.628932)\n",
      "  MAE Improvement: +50.90% (Baseline: 1.523584 → Enhanced: 0.748077)\n",
      "IMPROVEMENTS FOR HLF\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: -499.19% (Baseline: 0.0007 → Enhanced: -0.0028)\n",
      "  RMSE Improvement: -0.18% (Baseline: 14.506652 → Enhanced: 14.532319)\n",
      "  MAE Improvement: +27.21% (Baseline: 2.498308 → Enhanced: 1.818559)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -16.06% (Baseline: 0.0220 → Enhanced: 0.0185)\n",
      "  RMSE Improvement: -0.18% (Baseline: 14.351137 → Enhanced: 14.377068)\n",
      "  MAE Improvement: +2.70% (Baseline: 1.529973 → Enhanced: 1.488717)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: -499.19% (Baseline: 0.0007 → Enhanced: -0.0028)\n",
      "  RMSE Improvement: -0.18% (Baseline: 14.506652 → Enhanced: 14.532319)\n",
      "  MAE Improvement: +27.21% (Baseline: 2.498308 → Enhanced: 1.818559)\n",
      "IMPROVEMENTS FOR MDRX\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: -216.28% (Baseline: -0.0264 → Enhanced: -0.0834)\n",
      "  RMSE Improvement: -2.74% (Baseline: 1.039149 → Enhanced: 1.067646)\n",
      "  MAE Improvement: +3.13% (Baseline: 0.596701 → Enhanced: 0.578024)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -8.17% (Baseline: 0.3874 → Enhanced: 0.3558)\n",
      "  RMSE Improvement: -2.55% (Baseline: 0.802792 → Enhanced: 0.823267)\n",
      "  MAE Improvement: -13.72% (Baseline: 0.395447 → Enhanced: 0.449698)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: -216.28% (Baseline: -0.0264 → Enhanced: -0.0834)\n",
      "  RMSE Improvement: -2.74% (Baseline: 1.039149 → Enhanced: 1.067646)\n",
      "  MAE Improvement: +3.13% (Baseline: 0.596701 → Enhanced: 0.578024)\n",
      "IMPROVEMENTS FOR ORCL\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: +86.60% (Baseline: -0.4709 → Enhanced: -0.0631)\n",
      "  RMSE Improvement: +14.99% (Baseline: 0.853607 → Enhanced: 0.725692)\n",
      "  MAE Improvement: +16.61% (Baseline: 0.550036 → Enhanced: 0.458685)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -4.84% (Baseline: 0.5266 → Enhanced: 0.5011)\n",
      "  RMSE Improvement: -2.66% (Baseline: 0.484262 → Enhanced: 0.497125)\n",
      "  MAE Improvement: -1.43% (Baseline: 0.271508 → Enhanced: 0.275395)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: +86.60% (Baseline: -0.4709 → Enhanced: -0.0631)\n",
      "  RMSE Improvement: +14.99% (Baseline: 0.853607 → Enhanced: 0.725692)\n",
      "  MAE Improvement: +16.61% (Baseline: 0.550036 → Enhanced: 0.458685)\n",
      "IMPROVEMENTS FOR WFC\n",
      "\n",
      "Window-Based Forecasting:\n",
      "  R² Improvement: +31.96% (Baseline: -0.2541 → Enhanced: -0.1729)\n",
      "  RMSE Improvement: +3.29% (Baseline: 0.929697 → Enhanced: 0.899097)\n",
      "  MAE Improvement: +3.44% (Baseline: 0.634627 → Enhanced: 0.612791)\n",
      "\n",
      "Day-Ahead Forecasting:\n",
      "  R² Improvement: -9.01% (Baseline: 0.5040 → Enhanced: 0.4586)\n",
      "  RMSE Improvement: -4.48% (Baseline: 0.584698 → Enhanced: 0.610874)\n",
      "  MAE Improvement: -3.07% (Baseline: 0.373401 → Enhanced: 0.384861)\n",
      "\n",
      "Historical-Based Forecasting:\n",
      "  R² Improvement: +31.96% (Baseline: -0.2541 → Enhanced: -0.1729)\n",
      "  RMSE Improvement: +3.29% (Baseline: 0.929697 → Enhanced: 0.899097)\n",
      "  MAE Improvement: +3.44% (Baseline: 0.634627 → Enhanced: 0.612791)\n"
     ]
    }
   ],
   "source": [
    "for company in tickers.values():\n",
    "    print(f\"IMPROVEMENTS FOR {company}\")\n",
    "    for method in methods.keys():\n",
    "        print(f\"\\n{methods[method]}:\")\n",
    "        \n",
    "        baseline_metrics = metrics[company]['baseline'][method]\n",
    "        enhanced_metrics = metrics[company]['enhanced'][method]\n",
    "        \n",
    "        # Calculate R² improvement (higher is better, so enhanced - baseline)\n",
    "        if baseline_metrics['R2'] != 0:\n",
    "            r2_improvement = ((enhanced_metrics['R2'] - baseline_metrics['R2']) / \n",
    "                             abs(baseline_metrics['R2']) * 100)\n",
    "        else:\n",
    "            r2_improvement = 0\n",
    "        \n",
    "        # Calculate RMSE improvement (lower is better, so baseline - enhanced)\n",
    "        if baseline_metrics['RMSE'] != 0:\n",
    "            rmse_improvement = ((baseline_metrics['RMSE'] - enhanced_metrics['RMSE']) / \n",
    "                               baseline_metrics['RMSE'] * 100)\n",
    "        else:\n",
    "            rmse_improvement = 0\n",
    "        \n",
    "        # Calculate MAE improvement (lower is better, so baseline - enhanced)\n",
    "        if baseline_metrics['MAE'] != 0:\n",
    "            mae_improvement = ((baseline_metrics['MAE'] - enhanced_metrics['MAE']) / \n",
    "                              baseline_metrics['MAE'] * 100)\n",
    "        else:\n",
    "            mae_improvement = 0\n",
    "        \n",
    "        print(f\"  R² Improvement: {r2_improvement:+.2f}% (Baseline: {baseline_metrics['R2']:.4f} → Enhanced: {enhanced_metrics['R2']:.4f})\")\n",
    "        print(f\"  RMSE Improvement: {rmse_improvement:+.2f}% (Baseline: {baseline_metrics['RMSE']:.6f} → Enhanced: {enhanced_metrics['RMSE']:.6f})\")\n",
    "        print(f\"  MAE Improvement: {mae_improvement:+.2f}% (Baseline: {baseline_metrics['MAE']:.6f} → Enhanced: {enhanced_metrics['MAE']:.6f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f9b717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f1394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating visualization for AMSC...\n",
      "  ✓ Saved: data/output/AMSC_comparison.png\n",
      "\n",
      "Generating visualization for BP...\n",
      "  ✓ Saved: data/output/BP_comparison.png\n",
      "\n",
      "Generating visualization for EVR...\n",
      "  ✓ Saved: data/output/EVR_comparison.png\n",
      "\n",
      "Generating visualization for GOOGL...\n",
      "  ✓ Saved: data/output/GOOGL_comparison.png\n",
      "\n",
      "Generating visualization for HLF...\n",
      "  ✓ Saved: data/output/HLF_comparison.png\n",
      "\n",
      "Generating visualization for MDRX...\n",
      "  ✓ Saved: data/output/MDRX_comparison.png\n",
      "\n",
      "Generating visualization for ORCL...\n",
      "  ✓ Saved: data/output/ORCL_comparison.png\n",
      "\n",
      "Generating visualization for WFC...\n",
      "  ✓ Saved: data/output/WFC_comparison.png\n",
      "\n",
      "{'='*70}\n",
      "ALL VISUALIZATIONS COMPLETE\n",
      "{'='*70}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for company in tickers.values():\n",
    "    print(f\"\\nGenerating visualization for {company}...\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Extract per-method baseline/enhanced entries for THIS company\n",
    "    baseline_results = [r for r in results if r['Model'] == 'baseline' and r['Company'] == company]\n",
    "    enhanced_results = [r for r in results if r['Model'] == 'enhanced' and r['Company'] == company]\n",
    "    \n",
    "    # Skip if no data for this company\n",
    "    if len(baseline_results) == 0 or len(enhanced_results) == 0:\n",
    "        print(f\"  WARNING: No results found for {company}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # 1. R² Comparison -------------------------------------------------------------\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_r2 = [r['R2'] for r in baseline_results]\n",
    "    enhanced_r2 = [r['R2'] for r in enhanced_results]\n",
    "\n",
    "    ax1.bar(x - width/2, baseline_r2, width, label='Baseline', alpha=0.8, color='steelblue')\n",
    "    ax1.bar(x + width/2, enhanced_r2, width, label='Enhanced', alpha=0.8, color='coral')\n",
    "\n",
    "    ax1.set_ylabel('R² Score', fontsize=12)\n",
    "    ax1.set_title(f'{company} - R² Score Comparison by Method', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([r['Method_Name'] for r in baseline_results], rotation=15, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # 2. RMSE Comparison -----------------------------------------------------------\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "    baseline_rmse = [r['RMSE'] for r in baseline_results]\n",
    "    enhanced_rmse = [r['RMSE'] for r in enhanced_results]\n",
    "\n",
    "    ax2.bar(x - width/2, baseline_rmse, width, label='Baseline', alpha=0.8, color='steelblue')\n",
    "    ax2.bar(x + width/2, enhanced_rmse, width, label='Enhanced', alpha=0.8, color='coral')\n",
    "\n",
    "    ax2.set_ylabel('RMSE', fontsize=12)\n",
    "    ax2.set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(['Window', 'Day', 'Historical'], rotation=15, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 3–5. Prediction Plots per method --------------------------------------------\n",
    "    for idx, (baseline_entry, enhanced_entry) in enumerate(zip(baseline_results, enhanced_results)):\n",
    "        ax = fig.add_subplot(gs[1 + idx // 3, idx % 3])\n",
    "\n",
    "        actuals = baseline_entry['Actuals'][:200]\n",
    "        pred_baseline = baseline_entry['Predictions'][:200]\n",
    "        pred_enhanced = enhanced_entry['Predictions'][:200]\n",
    "\n",
    "        ax.plot(actuals, label='Actual', linewidth=2, alpha=0.7, color='black')\n",
    "        ax.plot(pred_baseline, label='Baseline', linewidth=1.5, alpha=0.7, color='steelblue')\n",
    "        ax.plot(pred_enhanced, label='Enhanced', linewidth=1.5, alpha=0.7, color='coral')\n",
    "\n",
    "        ax.set_xlabel('Time Step', fontsize=10)\n",
    "        ax.set_ylabel('Normalized Volume', fontsize=10)\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{baseline_entry['Method_Name']}\\n\"\n",
    "            f\"Base R²: {baseline_entry['R2']:.3f} | \"\n",
    "            f\"Enh R²: {enhanced_entry['R2']:.3f}\",\n",
    "            fontsize=11, fontweight='bold'\n",
    "        )\n",
    "\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'{company} - Baseline vs Enhanced Model Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    output_file = f'data/output/{company}_comparison.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  ✓ Saved: {output_file}\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n{'='*70}\")\n",
    "print(\"ALL VISUALIZATIONS COMPLETE\")\n",
    "print(\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
